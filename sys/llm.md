# LLM

### Papers

[Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)

[MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/pdf/2402.15627.pdf)

[Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://openreview.net/pdf?id=wIPIhHd00i)

[FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://openreview.net/pdf?id=RRntzKrBTp)



### Blogs

[大模型训练之序列并行双雄：DeepSpeed Ulysses & Ring-Attention](https://zhuanlan.zhihu.com/p/689067888)

[A100/H100 太贵，何不用 4090？](https://01.me/2023/09/h100-vs-4090/) [backup](https://zhuanlan.zhihu.com/p/655402388)

Retrieval-Augmented Generation [[IBM]](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) [[NV]](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)

[Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)

[漫谈高性能计算与性能优化：访存](https://zhuanlan.zhihu.com/p/600489819)

[漫谈高性能计算与性能优化：计算](https://zhuanlan.zhihu.com/p/688613416)

[How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU)

[cuda编程中，转为float4是什么？](https://www.zhihu.com/question/574968879/answer/3005751704)
