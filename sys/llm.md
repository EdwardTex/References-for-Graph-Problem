# LLM

### KV Cache

[Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/pdf/2310.01801)

[Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time](https://arxiv.org/pdf/2305.17118)

[KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/pdf/2401.18079)

[KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://arxiv.org/pdf/2402.02750)

[Layer-Condensed KV Cache for Efficient Inference of Large Language Models](https://arxiv.org/pdf/2405.10637)

[KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization](https://arxiv.org/pdf/2405.03917)

[A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression](https://arxiv.org/pdf/2406.11430)

[Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache](https://proceedings.mlsys.org/paper_files/paper/2024/file/bbb7506579431a85861a05fff048d3e1-Paper-Conference.pdf)


### General

[Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)

[LLM Serving - OSDI'24 Paper list](https://www.zhihu.com/question/649626302/answer/3440577248)

[Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/pdf/2402.17152)

### Training

[MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/pdf/2402.15627.pdf)


### Inference

[Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://openreview.net/pdf?id=wIPIhHd00i)

[FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://openreview.net/pdf?id=RRntzKrBTp)



### AI4EDA

[iEDA: An Open-source infrastructure of EDA](https://ieeexplore.ieee.org/document/10473983)

[LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation](https://arxiv.org/pdf/2401.12224)


### RAG

Retrieval-Augmented Generation [[IBM]](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) [[NV]](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)

[Survey github repo](https://github.com/hymie122/RAG-Survey)

[Fine-tuning LLMs for longer context and better RAG systems](https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems)

[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://www.microsoft.com/en-us/research/project/graphrag/overview/)

[2024年大模型LLM还有哪些可研究的方向？](https://www.zhihu.com/question/637595961/answer/3483115960)

[Azure AI Search : RAG 的最佳检索方式 - 混合检索+语义重排](https://mp.weixin.qq.com/s/NQbLQFUpxcjUhsD9fCDy1A)

[Vector databases and knowledge graphs: The cornerstones of effective RAG](https://www.exlservice.com/insights/white-paper/vector-databases-and-knowledge-graphs-the-cornerstones-of-effective-rag)

### Blogs

[Bojie Li - 我是如何走上 AI 创业之路的](https://zhuanlan.zhihu.com/p/689821495)

[Numbers every LLM Developer should know](https://github.com/ray-project/llm-numbers)

[为什么现在的LLM都是Decoder only的架构？](https://www.zhihu.com/question/588325646/answer/3405060345)

[大模型训练之序列并行双雄：DeepSpeed Ulysses & Ring-Attention](https://zhuanlan.zhihu.com/p/689067888)

[A100/H100 太贵，何不用 4090？](https://01.me/2023/09/h100-vs-4090/) [backup](https://zhuanlan.zhihu.com/p/655402388)

[Roofline Model与深度学习模型的性能分析](https://zhuanlan.zhihu.com/p/34204282)

[漫谈高性能计算与性能优化：访存](https://zhuanlan.zhihu.com/p/600489819)

[漫谈高性能计算与性能优化：计算](https://zhuanlan.zhihu.com/p/688613416)

[How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU)

[cuda编程中，转为float4是什么？](https://www.zhihu.com/question/574968879/answer/3005751704)
